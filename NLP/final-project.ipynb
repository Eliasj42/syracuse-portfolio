{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHANGE PARAMETERS HERE ###\n",
    "REMOVE_STOPWORDS = False\n",
    "USE_WORD_COUNTS = True\n",
    "USE_PERCENT_NEW_WORDS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "def read_spam_file(fname, isspam, clean = REMOVE_STOPWORDS):\n",
    "    try:\n",
    "        f_ = open(fname, 'r')\n",
    "        email = f_.read()\n",
    "        f_.close()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    tokens = word_tokenize(email)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    if clean:\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return (tokens, isspam)\n",
    "\n",
    "# Create a dataset of all emails from both spam and ham folders\n",
    "def create_all_emails():\n",
    "    ham_emails = [read_spam_file('FinalProjectData/EmailSpamCorpora/corpus/ham/' + fname, False) \n",
    "              for fname in os.listdir('FinalProjectData/EmailSpamCorpora/corpus/ham/')]\n",
    "    ham_emails = [x for x in ham_emails if x is not None]\n",
    "    spam_emails = [read_spam_file('FinalProjectData/EmailSpamCorpora/corpus/spam/' + fname, True) \n",
    "               for fname in os.listdir('FinalProjectData/EmailSpamCorpora/corpus/spam/')]\n",
    "    spam_emails = [x for x in spam_emails if x is not None]\n",
    "    all_emails = ham_emails + spam_emails\n",
    "    random.shuffle(all_emails)\n",
    "    return all_emails\n",
    "\n",
    "all_emails = create_all_emails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = [word for (sent,cat) in all_emails for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(3000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "\n",
    "def percent_new_words(email, word_features):\n",
    "    # Calculated the percentage of words not in the top 3000\n",
    "    email_words = set(email)\n",
    "    not_in = 0\n",
    "    total = 0\n",
    "    for word in word_features:\n",
    "        total += 1\n",
    "        if not word in email_words:\n",
    "            not_in += 1\n",
    "    return not_in / total\n",
    "\n",
    "# Turn each email into a feature set\n",
    "def email_features(email, use_counts, word_features = word_features, new_feature1 = False):\n",
    "    email_words = set(email)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        if not use_counts: features['V_{}'.format(word)] = int(word in email_words)\n",
    "        if use_counts: features['V_{}'.format(word)] = email.count(word)\n",
    "    result = list(features.values())\n",
    "    if new_feature1:\n",
    "        result += [percent_new_words(email, word_features)]\n",
    "    return np.asarray(result)\n",
    "\n",
    "X = [email_features(email, \n",
    "                    USE_WORD_COUNTS, \n",
    "                    new_feature1 = USE_PERCENT_NEW_WORDS) for (email, _) in all_emails]\n",
    "Y = [int(spam) for (_, spam) in all_emails]\n",
    "\n",
    "assert(len(X) == len(Y))\n",
    "\n",
    "cutoff = int(len(X) * 0.9)\n",
    "\n",
    "Xtrain = np.asarray(X[:cutoff])\n",
    "Xtest = np.asarray(X[cutoff:])\n",
    "Ytrain = np.asarray(Y[:cutoff])\n",
    "Ytest = np.asarray(Y[cutoff:])\n",
    "\n",
    "# Create folds for cross validation\n",
    "fold_size = int(len(X)/10)\n",
    "Xfolds = []\n",
    "Yfolds = []\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        Xfolds.append(X[:fold_size])\n",
    "        Yfolds.append(Y[:fold_size])\n",
    "    elif i == 9:\n",
    "        Xfolds.append(X[9 * fold_size:])\n",
    "        Yfolds.append(Y[9 * fold_size:])\n",
    "    else:\n",
    "        Xfolds.append(X[i * fold_size:(i + 1) * fold_size])\n",
    "        Yfolds.append(Y[i * fold_size:(i + 1) * fold_size]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Naive Bayes with Cross validation\n",
    "def Naive_Bayes_with_CV(Xfolds, Yfolds):\n",
    "    gnb = GaussianNB()\n",
    "    acc = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    fstat = []\n",
    "    for i in range(len(Xfolds)):\n",
    "        xtrain = []\n",
    "        ytrain = []\n",
    "        xtest = Xfolds[i]\n",
    "        ytest = Yfolds[i]\n",
    "        for j in range(len(Xfolds)):\n",
    "            if j != i:\n",
    "                xtrain += Xfolds[j]\n",
    "                ytrain += Yfolds[j]\n",
    "        xtrain = np.asarray(xtrain)\n",
    "        ytrain = np.asarray(ytrain)\n",
    "        xtest = np.asarray(xtest)\n",
    "        ytest = np.asarray(ytest)\n",
    "        y_pred = gnb.fit(xtrain, ytrain).predict(xtest)\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        n = 0\n",
    "        for k in range(len(y_pred)):\n",
    "            if y_pred[k] == 1 and ytest[k] == 1: tp += 1\n",
    "            elif y_pred[k] == 0 and ytest[k] == 0: tn += 1\n",
    "            elif y_pred[k] == 0 and ytest[k] == 1: fn += 1\n",
    "            elif y_pred[k] == 1 and ytest[k] == 0: fp += 1\n",
    "            n += 1\n",
    "        acc.append((tn + tp) / n)\n",
    "        prec.append(tp / (tp + fp))\n",
    "        rec.append(tp / (tp + fn))\n",
    "        fstat.append(2 * (((tp / (tp + fp)) * (tp / (tp + fn)))/((tp / (tp + fp)) + (tp / (tp + fn)))))\n",
    "    return np.asarray(acc), np.asarray(prec), np.asarray(rec), np.asarray(fstat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961820481345088\n",
      "Precision: 0.9068299664699655\n",
      "Recall: 0.9680346423545213\n",
      "F-measure: 0.9360256537912036\n"
     ]
    }
   ],
   "source": [
    "# Print Accuracy Statistics\n",
    "accuracy, precision, recall, fmeasure = Naive_Bayes_with_CV(Xfolds, Yfolds)\n",
    "print('Accuracy:', np.mean(accuracy))\n",
    "print('Precision:', np.mean(precision))\n",
    "print('Recall:', np.mean(recall))\n",
    "print('F-measure:', np.mean(fmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "209/209 [==============================] - 16s 72ms/step - loss: 0.5076 - accuracy: 0.8379 - val_loss: 0.0799 - val_accuracy: 0.9763\n",
      "Epoch 2/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.1155 - accuracy: 0.9804 - val_loss: 0.0347 - val_accuracy: 0.9892\n",
      "Epoch 3/25\n",
      "209/209 [==============================] - 15s 70ms/step - loss: 0.0542 - accuracy: 0.9822 - val_loss: 0.0628 - val_accuracy: 0.9828\n",
      "Epoch 4/25\n",
      "209/209 [==============================] - 15s 72ms/step - loss: 0.1356 - accuracy: 0.9845 - val_loss: 0.0715 - val_accuracy: 0.9806\n",
      "Epoch 5/25\n",
      "209/209 [==============================] - 14s 65ms/step - loss: 0.0377 - accuracy: 0.9899 - val_loss: 0.0699 - val_accuracy: 0.9849\n",
      "Epoch 6/25\n",
      "209/209 [==============================] - 14s 65ms/step - loss: 0.0520 - accuracy: 0.9899 - val_loss: 0.1096 - val_accuracy: 0.9785\n",
      "Epoch 7/25\n",
      "209/209 [==============================] - 14s 66ms/step - loss: 0.0690 - accuracy: 0.9886 - val_loss: 0.0921 - val_accuracy: 0.9806\n",
      "Epoch 8/25\n",
      "209/209 [==============================] - 14s 66ms/step - loss: 0.0489 - accuracy: 0.9905 - val_loss: 0.1282 - val_accuracy: 0.9806\n",
      "Epoch 9/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0265 - accuracy: 0.9929 - val_loss: 0.2131 - val_accuracy: 0.9806\n",
      "Epoch 10/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0308 - accuracy: 0.9963 - val_loss: 0.1164 - val_accuracy: 0.9785\n",
      "Epoch 11/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0193 - accuracy: 0.9953 - val_loss: 0.1320 - val_accuracy: 0.9849\n",
      "Epoch 12/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0623 - val_accuracy: 0.9849\n",
      "Epoch 13/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.0656 - val_accuracy: 0.9828\n",
      "Epoch 14/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0128 - accuracy: 0.9974 - val_loss: 0.1841 - val_accuracy: 0.9849\n",
      "Epoch 15/25\n",
      "209/209 [==============================] - 14s 68ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.0626 - val_accuracy: 0.9785\n",
      "Epoch 16/25\n",
      "209/209 [==============================] - 14s 66ms/step - loss: 0.0087 - accuracy: 0.9981 - val_loss: 0.1896 - val_accuracy: 0.9892\n",
      "Epoch 17/25\n",
      "209/209 [==============================] - 14s 66ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0538 - val_accuracy: 0.9849\n",
      "Epoch 18/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0658 - accuracy: 0.9947 - val_loss: 0.2263 - val_accuracy: 0.9871\n",
      "Epoch 19/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0468 - accuracy: 0.9956 - val_loss: 0.1031 - val_accuracy: 0.9849\n",
      "Epoch 20/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0195 - accuracy: 0.9973 - val_loss: 0.0645 - val_accuracy: 0.9828\n",
      "Epoch 21/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0385 - accuracy: 0.9970 - val_loss: 0.1404 - val_accuracy: 0.9742\n",
      "Epoch 22/25\n",
      "209/209 [==============================] - 14s 68ms/step - loss: 0.0465 - accuracy: 0.9909 - val_loss: 0.0906 - val_accuracy: 0.9849\n",
      "Epoch 23/25\n",
      "209/209 [==============================] - 14s 67ms/step - loss: 0.0374 - accuracy: 0.9979 - val_loss: 0.1093 - val_accuracy: 0.9763\n",
      "Epoch 24/25\n",
      " 32/209 [===>..........................] - ETA: 11s - loss: 0.0930 - accuracy: 0.9864"
     ]
    }
   ],
   "source": [
    "# Define a NN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(3000, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.75))\n",
    "model.add(tf.keras.layers.Dense(1000, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(250, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(1000, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(Xtrain, to_categorical(Ytrain), epochs = 25, batch_size = 20, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Accuracy measures\n",
    "Ypred = model.predict(Xtest)\n",
    "Ypred = np.argmax(Ypred, axis = 1)\n",
    "Ytest = Ytest\n",
    "print('Accuracy:', accuracy_score(Ytest, Ypred))\n",
    "print('Precision:',precision_score(Ytest, Ypred))\n",
    "print('Recall:',recall_score(Ytest, Ypred))\n",
    "print('F-Measure:',f1_score(Ytest, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None, None,\n",
       "       None, None, None, None, None, None, None, None, None, None],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
